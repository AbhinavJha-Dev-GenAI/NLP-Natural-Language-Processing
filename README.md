# NLP - Natural Language Processing ðŸ“

**Target Level**: 2-3 Year ML/AI Engineer  
**Priority**: ðŸ—£ï¸ **HIGH**

---

## ðŸ“š What You'll Learn
The bridge between human language and machine understanding, from Regex to Transformers.

### Core Topics
- ✅ **[01. Text Preprocessing](01-Text-Preprocessing/README.md)**: Cleaning, Normalization, Lemmatization.
- ✅ **[02. Classical NLP](02-Classical-NLP/README.md)**: Tokenization, Stemming, TF-IDF, N-grams.
- ✅ **[03. Word Embeddings](03-Word-Embeddings/README.md)**: Word2Vec, GloVe, FastText.
- ✅ **[04. Sequence Models](04-Sequence-Models/README.md)**: RNNs, LSTMs, and Seq2Seq.
- ✅ **[05. Attention Mechanisms](05-Attention-Mechanisms/README.md)**: Global vs Self-Attention.
- ✅ **[06. Transformers](06-Transformers/README.md)**: The architecture that changed everything.
- ✅ **[07. Tokenization](07-Tokenization/README.md)**: BPE, WordPiece, SentencePiece.
- ✅ **[08. NLP Tasks](08-NLP-Tasks/README.md)**: Sentiment, NER, Summarization, QA.
- ✅ **[09. Fine-Tuning](09-Fine-Tuning/README.md)**: Transfer Learning and Optimization.
- ✅ **[10. Advanced Embeddings](10-Embeddings-Advanced/README.md)**: BERT, RoBERTa, Vector DBs.
- ✅ **[11. Projects](11-Projects/README.md)**: Semantic Search, Sentiment Dashboard.
- ✅ **[12. Research Papers](12-Research-Papers/README.md)**: From Attention to GPT-3.
- ✅ **[13. Interview Prep](13-Interview-Prep/README.md)**: Theory, Scenarios, and FAQ.

---

## ðŸŽ¯ Learning Path
1. **Text Preprocessing**: Clean and prepare messy real-world text.
2. **Static Embeddings**: Learn vector representation of words.
3. **Attention is All You Need**: Deep dive into the Transformer architecture.
4. **Task Mastery**: Solve NER, Summarization, and Translation tasks.

---

## ðŸ› ï¸ Tools
- Hugging Face, spaCy, NLTK, Gensim

---

Happy Processing! ðŸš€
