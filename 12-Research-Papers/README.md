# 12. NLP Research Papers ðŸ“š

Foundation and state-of-the-art papers that every NLP engineer should know.

## Foundational
1.  **Efficient Estimation of Word Representations in Vector Space (2013)**: Introducing Word2Vec. [Read Paper](https://arxiv.org/abs/1301.3781)
2.  **GloVe: Global Vectors for Word Representation (2014)**: Stanford's approach to embeddings. [Read Paper](https://nlp.stanford.edu/pubs/glove.pdf)

## The Transformer Era
3.  **Attention Is All You Need (2017)**: The most influential paper in modern NLP. Introduction of the Transformer. [Read Paper](https://arxiv.org/abs/1706.03762)
4.  **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)**: Fine-tuning for multiple tasks. [Read Paper](https://arxiv.org/abs/1810.04805)
5.  **Language Models are Few-Shot Learners (2020)**: Introduction of GPT-3. [Read Paper](https://arxiv.org/abs/2005.14165)

## Specialized
6.  **Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (2019)**: T5 model paper. [Read Paper](https://arxiv.org/abs/1910.10683)
7.  **LoRA: Low-Rank Adaptation of Large Language Models (2021)**: Parameter-efficient fine-tuning. [Read Paper](https://arxiv.org/abs/2106.09685)

---

> [!TIP]
> Use **Connected Papers** or **ResearchRabbit** to find papers that cited these seminal works to stay up to date with the latest SOTA.
